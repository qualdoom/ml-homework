{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r-Mb7bnoaRLM"
   },
   "source": [
    "# Логичтическая регрессия, метод опорных векторов, one-hot кодирование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNyXo3CvaRLP"
   },
   "source": [
    "### О задании\n",
    "\n",
    "В этом задании вы:\n",
    "- настроите метод опорных векторов\n",
    "- изучите методы работы с категориальными переменными"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "ndj090dOaRLQ",
    "outputId": "fdef6337-1efe-4945-d445-fb17c46dcc4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1_wS0x7RaRLR"
   },
   "source": [
    "__Задание 1.__ Обучение логистической регрессии на реальных данных и оценка качества классификации.\n",
    "\n",
    "**(5 баллов)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "94eHa5RgaRLS"
   },
   "source": [
    "Загрузим данные с конкурса [Kaggle Porto Seguro’s Safe Driver Prediction](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction) (вам нужна только обучающая выборка). Задача состоит в определении водителей, которые в ближайший год воспользуются своей автомобильной страховкой (бинарная классификация). Но для нас важна будет не сама задача, а только её данные. При этом под нужды задания мы немного модифицируем датасет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true,
    "id": "BJQn-94DaRLS"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ps_ind_01</th>\n",
       "      <th>ps_ind_02_cat</th>\n",
       "      <th>ps_ind_03</th>\n",
       "      <th>ps_ind_04_cat</th>\n",
       "      <th>ps_ind_05_cat</th>\n",
       "      <th>ps_ind_06_bin</th>\n",
       "      <th>ps_ind_07_bin</th>\n",
       "      <th>ps_ind_08_bin</th>\n",
       "      <th>ps_ind_09_bin</th>\n",
       "      <th>ps_ind_10_bin</th>\n",
       "      <th>...</th>\n",
       "      <th>ps_calc_11</th>\n",
       "      <th>ps_calc_12</th>\n",
       "      <th>ps_calc_13</th>\n",
       "      <th>ps_calc_14</th>\n",
       "      <th>ps_calc_15_bin</th>\n",
       "      <th>ps_calc_16_bin</th>\n",
       "      <th>ps_calc_17_bin</th>\n",
       "      <th>ps_calc_18_bin</th>\n",
       "      <th>ps_calc_19_bin</th>\n",
       "      <th>ps_calc_20_bin</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ps_ind_01  ps_ind_02_cat  ps_ind_03  ps_ind_04_cat  ps_ind_05_cat  \\\n",
       "id                                                                      \n",
       "7           2              2          5              1              0   \n",
       "9           1              1          7              0              0   \n",
       "13          5              4          9              1              0   \n",
       "16          0              1          2              0              0   \n",
       "17          0              2          0              1              0   \n",
       "\n",
       "    ps_ind_06_bin  ps_ind_07_bin  ps_ind_08_bin  ps_ind_09_bin  ps_ind_10_bin  \\\n",
       "id                                                                              \n",
       "7               0              1              0              0              0   \n",
       "9               0              0              1              0              0   \n",
       "13              0              0              1              0              0   \n",
       "16              1              0              0              0              0   \n",
       "17              1              0              0              0              0   \n",
       "\n",
       "    ...  ps_calc_11  ps_calc_12  ps_calc_13  ps_calc_14  ps_calc_15_bin  \\\n",
       "id  ...                                                                   \n",
       "7   ...           9           1           5           8               0   \n",
       "9   ...           3           1           1           9               0   \n",
       "13  ...           4           2           7           7               0   \n",
       "16  ...           2           2           4           9               0   \n",
       "17  ...           3           1           1           3               0   \n",
       "\n",
       "    ps_calc_16_bin  ps_calc_17_bin  ps_calc_18_bin  ps_calc_19_bin  \\\n",
       "id                                                                   \n",
       "7                1               1               0               0   \n",
       "9                1               1               0               1   \n",
       "13               1               1               0               1   \n",
       "16               0               0               0               0   \n",
       "17               0               0               1               1   \n",
       "\n",
       "    ps_calc_20_bin  \n",
       "id                  \n",
       "7                1  \n",
       "9                0  \n",
       "13               0  \n",
       "16               0  \n",
       "17               0  \n",
       "\n",
       "[5 rows x 57 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('C:\\\\Users\\\\vinik\\\\Githubik\\\\ml-homework\\\\task6_classification\\\\porto-seguro-safe-driver-prediction\\\\train.csv', index_col=0)\n",
    "target = data.target.values\n",
    "data = data.drop('target', axis=1)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u2Su7GNhaRLT"
   },
   "source": [
    "Пересемплируем выборку так, чтобы положительных и отрицательных объектов в выборке было одинаковое число. Разделим на обучающую и тестовую выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true,
    "id": "fot9A7L8aRLT"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "np.random.seed(910)\n",
    "mask_plus = np.random.choice(np.where(target == 1)[0], 100000, replace=True)\n",
    "mask_zero = np.random.choice(np.where(target == 0)[0], 100000, replace=True)\n",
    "\n",
    "data = pd.concat((data.iloc[mask_plus], data.iloc[mask_zero]))\n",
    "target = np.hstack((target[mask_plus], target[mask_zero]))\n",
    "\n",
    "target_pd = pd.DataFrame({'target': target})\n",
    "\n",
    "data = data.reset_index(drop=True)\n",
    "target_pd = target_pd.reset_index(drop=True)\n",
    "\n",
    "df = pd.concat([data, target_pd], axis=1)\n",
    "\n",
    "shuffled_df = df.sample(frac=1, random_state=42).reset_index(drop=True)  # frac=1 means all rows are shuffled\n",
    "\n",
    "# Split the shuffled list back into data and target\n",
    "\n",
    "shuffled_data = shuffled_df.drop('target', axis=1).reset_index(drop=True)\n",
    "shuffled_target = np.array(shuffled_df['target'])\n",
    "\n",
    "data = shuffled_data\n",
    "target = shuffled_target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.5, random_state=46)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6GB0kVSoaRLT"
   },
   "source": [
    "Не забудьте отнормировать признаки (можно воспользоваться StandardScaler или сделать это вручную). Пока не будем обращать внимание на то, что некоторые признаки категориальные (этим мы займёмся позже)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true,
    "id": "5dDctZhDaRLU"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ps_ind_01</th>\n",
       "      <th>ps_ind_02_cat</th>\n",
       "      <th>ps_ind_03</th>\n",
       "      <th>ps_ind_04_cat</th>\n",
       "      <th>ps_ind_05_cat</th>\n",
       "      <th>ps_ind_06_bin</th>\n",
       "      <th>ps_ind_07_bin</th>\n",
       "      <th>ps_ind_08_bin</th>\n",
       "      <th>ps_ind_09_bin</th>\n",
       "      <th>ps_ind_10_bin</th>\n",
       "      <th>...</th>\n",
       "      <th>ps_calc_11</th>\n",
       "      <th>ps_calc_12</th>\n",
       "      <th>ps_calc_13</th>\n",
       "      <th>ps_calc_14</th>\n",
       "      <th>ps_calc_15_bin</th>\n",
       "      <th>ps_calc_16_bin</th>\n",
       "      <th>ps_calc_17_bin</th>\n",
       "      <th>ps_calc_18_bin</th>\n",
       "      <th>ps_calc_19_bin</th>\n",
       "      <th>ps_calc_20_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.000000e+05</td>\n",
       "      <td>2.000000e+05</td>\n",
       "      <td>2.000000e+05</td>\n",
       "      <td>2.000000e+05</td>\n",
       "      <td>2.000000e+05</td>\n",
       "      <td>2.000000e+05</td>\n",
       "      <td>2.000000e+05</td>\n",
       "      <td>2.000000e+05</td>\n",
       "      <td>2.000000e+05</td>\n",
       "      <td>2.000000e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>2.000000e+05</td>\n",
       "      <td>2.000000e+05</td>\n",
       "      <td>2.000000e+05</td>\n",
       "      <td>2.000000e+05</td>\n",
       "      <td>2.000000e+05</td>\n",
       "      <td>2.000000e+05</td>\n",
       "      <td>2.000000e+05</td>\n",
       "      <td>2.000000e+05</td>\n",
       "      <td>2.000000e+05</td>\n",
       "      <td>2.000000e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-1.918465e-17</td>\n",
       "      <td>9.514167e-17</td>\n",
       "      <td>1.596234e-16</td>\n",
       "      <td>3.017142e-17</td>\n",
       "      <td>-1.822542e-17</td>\n",
       "      <td>-4.645173e-17</td>\n",
       "      <td>-2.959410e-17</td>\n",
       "      <td>8.718359e-17</td>\n",
       "      <td>-1.006129e-16</td>\n",
       "      <td>7.105427e-20</td>\n",
       "      <td>...</td>\n",
       "      <td>1.184475e-16</td>\n",
       "      <td>-3.794298e-17</td>\n",
       "      <td>-4.554579e-17</td>\n",
       "      <td>-4.089173e-17</td>\n",
       "      <td>7.460699e-19</td>\n",
       "      <td>5.782042e-17</td>\n",
       "      <td>-2.359002e-17</td>\n",
       "      <td>-6.554757e-18</td>\n",
       "      <td>-2.522427e-18</td>\n",
       "      <td>2.739142e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>1.000003e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-9.905914e-01</td>\n",
       "      <td>-3.509020e+00</td>\n",
       "      <td>-1.636898e+00</td>\n",
       "      <td>-2.878048e+00</td>\n",
       "      <td>-1.000223e+00</td>\n",
       "      <td>-7.355743e-01</td>\n",
       "      <td>-6.474524e-01</td>\n",
       "      <td>-4.620329e-01</td>\n",
       "      <td>-4.647157e-01</td>\n",
       "      <td>-2.168458e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.323999e+00</td>\n",
       "      <td>-1.201959e+00</td>\n",
       "      <td>-1.697466e+00</td>\n",
       "      <td>-2.748155e+00</td>\n",
       "      <td>-3.751168e-01</td>\n",
       "      <td>-1.307329e+00</td>\n",
       "      <td>-1.113178e+00</td>\n",
       "      <td>-6.352699e-01</td>\n",
       "      <td>-7.257531e-01</td>\n",
       "      <td>-4.246867e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-9.905914e-01</td>\n",
       "      <td>-5.435048e-01</td>\n",
       "      <td>-9.067708e-01</td>\n",
       "      <td>-8.647089e-01</td>\n",
       "      <td>-3.344130e-01</td>\n",
       "      <td>-7.355743e-01</td>\n",
       "      <td>-6.474524e-01</td>\n",
       "      <td>-4.620329e-01</td>\n",
       "      <td>-4.647157e-01</td>\n",
       "      <td>-2.168458e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.163894e-01</td>\n",
       "      <td>-3.694304e-01</td>\n",
       "      <td>-5.160407e-01</td>\n",
       "      <td>-5.625855e-01</td>\n",
       "      <td>-3.751168e-01</td>\n",
       "      <td>-1.307329e+00</td>\n",
       "      <td>-1.113178e+00</td>\n",
       "      <td>-6.352699e-01</td>\n",
       "      <td>-7.257531e-01</td>\n",
       "      <td>-4.246867e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-4.948532e-01</td>\n",
       "      <td>-5.435048e-01</td>\n",
       "      <td>-1.766434e-01</td>\n",
       "      <td>-8.647089e-01</td>\n",
       "      <td>-3.344130e-01</td>\n",
       "      <td>-7.355743e-01</td>\n",
       "      <td>-6.474524e-01</td>\n",
       "      <td>-4.620329e-01</td>\n",
       "      <td>-4.647157e-01</td>\n",
       "      <td>-2.168458e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.894870e-01</td>\n",
       "      <td>-3.694304e-01</td>\n",
       "      <td>7.467198e-02</td>\n",
       "      <td>-1.983240e-01</td>\n",
       "      <td>-3.751168e-01</td>\n",
       "      <td>7.649183e-01</td>\n",
       "      <td>8.983289e-01</td>\n",
       "      <td>-6.352699e-01</td>\n",
       "      <td>-7.257531e-01</td>\n",
       "      <td>-4.246867e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.966230e-01</td>\n",
       "      <td>9.392528e-01</td>\n",
       "      <td>9.185477e-01</td>\n",
       "      <td>1.148630e+00</td>\n",
       "      <td>-3.344130e-01</td>\n",
       "      <td>1.359482e+00</td>\n",
       "      <td>1.544515e+00</td>\n",
       "      <td>-4.620329e-01</td>\n",
       "      <td>-4.647157e-01</td>\n",
       "      <td>-2.168458e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>6.643178e-01</td>\n",
       "      <td>4.630982e-01</td>\n",
       "      <td>6.653846e-01</td>\n",
       "      <td>5.301991e-01</td>\n",
       "      <td>-3.751168e-01</td>\n",
       "      <td>7.649183e-01</td>\n",
       "      <td>8.983289e-01</td>\n",
       "      <td>1.574134e+00</td>\n",
       "      <td>1.377879e+00</td>\n",
       "      <td>-4.246867e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.479576e+00</td>\n",
       "      <td>3.904768e+00</td>\n",
       "      <td>2.378802e+00</td>\n",
       "      <td>1.148630e+00</td>\n",
       "      <td>3.660446e+00</td>\n",
       "      <td>1.359482e+00</td>\n",
       "      <td>1.544515e+00</td>\n",
       "      <td>2.164348e+00</td>\n",
       "      <td>2.151853e+00</td>\n",
       "      <td>4.611572e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>5.360244e+00</td>\n",
       "      <td>5.458269e+00</td>\n",
       "      <td>5.981798e+00</td>\n",
       "      <td>5.265599e+00</td>\n",
       "      <td>2.665836e+00</td>\n",
       "      <td>7.649183e-01</td>\n",
       "      <td>8.983289e-01</td>\n",
       "      <td>1.574134e+00</td>\n",
       "      <td>1.377879e+00</td>\n",
       "      <td>2.354677e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ps_ind_01  ps_ind_02_cat     ps_ind_03  ps_ind_04_cat  \\\n",
       "count  2.000000e+05   2.000000e+05  2.000000e+05   2.000000e+05   \n",
       "mean  -1.918465e-17   9.514167e-17  1.596234e-16   3.017142e-17   \n",
       "std    1.000003e+00   1.000003e+00  1.000003e+00   1.000003e+00   \n",
       "min   -9.905914e-01  -3.509020e+00 -1.636898e+00  -2.878048e+00   \n",
       "25%   -9.905914e-01  -5.435048e-01 -9.067708e-01  -8.647089e-01   \n",
       "50%   -4.948532e-01  -5.435048e-01 -1.766434e-01  -8.647089e-01   \n",
       "75%    4.966230e-01   9.392528e-01  9.185477e-01   1.148630e+00   \n",
       "max    2.479576e+00   3.904768e+00  2.378802e+00   1.148630e+00   \n",
       "\n",
       "       ps_ind_05_cat  ps_ind_06_bin  ps_ind_07_bin  ps_ind_08_bin  \\\n",
       "count   2.000000e+05   2.000000e+05   2.000000e+05   2.000000e+05   \n",
       "mean   -1.822542e-17  -4.645173e-17  -2.959410e-17   8.718359e-17   \n",
       "std     1.000003e+00   1.000003e+00   1.000003e+00   1.000003e+00   \n",
       "min    -1.000223e+00  -7.355743e-01  -6.474524e-01  -4.620329e-01   \n",
       "25%    -3.344130e-01  -7.355743e-01  -6.474524e-01  -4.620329e-01   \n",
       "50%    -3.344130e-01  -7.355743e-01  -6.474524e-01  -4.620329e-01   \n",
       "75%    -3.344130e-01   1.359482e+00   1.544515e+00  -4.620329e-01   \n",
       "max     3.660446e+00   1.359482e+00   1.544515e+00   2.164348e+00   \n",
       "\n",
       "       ps_ind_09_bin  ps_ind_10_bin  ...    ps_calc_11    ps_calc_12  \\\n",
       "count   2.000000e+05   2.000000e+05  ...  2.000000e+05  2.000000e+05   \n",
       "mean   -1.006129e-16   7.105427e-20  ...  1.184475e-16 -3.794298e-17   \n",
       "std     1.000003e+00   1.000003e+00  ...  1.000003e+00  1.000003e+00   \n",
       "min    -4.647157e-01  -2.168458e-02  ... -2.323999e+00 -1.201959e+00   \n",
       "25%    -4.647157e-01  -2.168458e-02  ... -6.163894e-01 -3.694304e-01   \n",
       "50%    -4.647157e-01  -2.168458e-02  ... -1.894870e-01 -3.694304e-01   \n",
       "75%    -4.647157e-01  -2.168458e-02  ...  6.643178e-01  4.630982e-01   \n",
       "max     2.151853e+00   4.611572e+01  ...  5.360244e+00  5.458269e+00   \n",
       "\n",
       "         ps_calc_13    ps_calc_14  ps_calc_15_bin  ps_calc_16_bin  \\\n",
       "count  2.000000e+05  2.000000e+05    2.000000e+05    2.000000e+05   \n",
       "mean  -4.554579e-17 -4.089173e-17    7.460699e-19    5.782042e-17   \n",
       "std    1.000003e+00  1.000003e+00    1.000003e+00    1.000003e+00   \n",
       "min   -1.697466e+00 -2.748155e+00   -3.751168e-01   -1.307329e+00   \n",
       "25%   -5.160407e-01 -5.625855e-01   -3.751168e-01   -1.307329e+00   \n",
       "50%    7.467198e-02 -1.983240e-01   -3.751168e-01    7.649183e-01   \n",
       "75%    6.653846e-01  5.301991e-01   -3.751168e-01    7.649183e-01   \n",
       "max    5.981798e+00  5.265599e+00    2.665836e+00    7.649183e-01   \n",
       "\n",
       "       ps_calc_17_bin  ps_calc_18_bin  ps_calc_19_bin  ps_calc_20_bin  \n",
       "count    2.000000e+05    2.000000e+05    2.000000e+05    2.000000e+05  \n",
       "mean    -2.359002e-17   -6.554757e-18   -2.522427e-18    2.739142e-17  \n",
       "std      1.000003e+00    1.000003e+00    1.000003e+00    1.000003e+00  \n",
       "min     -1.113178e+00   -6.352699e-01   -7.257531e-01   -4.246867e-01  \n",
       "25%     -1.113178e+00   -6.352699e-01   -7.257531e-01   -4.246867e-01  \n",
       "50%      8.983289e-01   -6.352699e-01   -7.257531e-01   -4.246867e-01  \n",
       "75%      8.983289e-01    1.574134e+00    1.377879e+00   -4.246867e-01  \n",
       "max      8.983289e-01    1.574134e+00    1.377879e+00    2.354677e+00  \n",
       "\n",
       "[8 rows x 57 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "start_data = data.copy()\n",
    "\n",
    "def normalize_data(data): #dont forget to use this!!!\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(data)\n",
    "    scaled_data = pd.DataFrame(scaled_data, columns=data.columns)\n",
    "    return scaled_data\n",
    "\n",
    "scaled_data = normalize_data(data)\n",
    "scaled_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mSrjeimpaRLU"
   },
   "source": [
    "Обучите логистическую регрессию с удобными для вас параметрами, примените регуляризацию. Сделайте предсказание на тестовой части выборки. Посчитайте accuracy, precision, recall и F меру"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true,
    "id": "A1tEHyNFaRLU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy =  0.59075 precision =  0.6002479824237019 recall =  0.5503170994375972 F_score =  0.5741991197860851\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_data, target, test_size=0.5, random_state=46)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "prediction = model.predict(X_test)\n",
    "\n",
    "def get_info_prediction(prediction, y_test):\n",
    "    assert len(prediction) == len(y_test)\n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "\n",
    "    for i in range(len(prediction)):\n",
    "        if prediction[i] == 0 and y_test[i] == 0:\n",
    "            TN += 1\n",
    "        if prediction[i] == 1 and y_test[i] == 1:\n",
    "            TP += 1\n",
    "        if prediction[i] == 0 and y_test[i] == 1:\n",
    "            FN += 1\n",
    "        if prediction[i] == 1 and y_test[i] == 0:\n",
    "            FP += 1\n",
    "\n",
    "    accuracy = (TN + TP) / (TN + TP + FP + FN)\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "\n",
    "    F_score = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    return accuracy, precision, recall, F_score\n",
    "\n",
    "accuracy, precision, recall, F_score = get_info_prediction(prediction, y_test)\n",
    "\n",
    "print(\"accuracy = \", accuracy, \"precision = \", precision, \"recall = \", recall, \"F_score = \", F_score)\n",
    "# to-do: write conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N9UQ31XFaRLU"
   },
   "source": [
    "__Выводы__ в свободной форме: Без использования какой-то особой нормализации данных с помошью логистической регрессии мы получаем accuracy ~ 59%, precision ~ 60% и recall ~ 55% и F_score ~ 0.574, что неплохо, лучше чем подбрасывание монетки!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lHvrjAQnaRLW"
   },
   "source": [
    "## Часть 2. Работа с категориальными переменными"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_doKeEKxaRLW"
   },
   "source": [
    "В этой части мы научимся обрабатывать категориальные переменные, так как закодировать их в виде чисел недостаточно (это задаёт некоторый порядок, которого на категориальных переменных может и не быть). Существует два основных способа обработки категориальных значений:\n",
    "- One-hot-кодирование\n",
    "- Счётчики (CTR, mean-target кодирование, ...) — каждый категориальный признак заменяется на среднее значение целевой переменной по всем объектам, имеющим одинаковое значение в этом признаке.\n",
    "\n",
    "Начнём с one-hot-кодирования. Допустим наш категориальный признак $f_j(x)$ принимает значения из множества $C=\\{c_1, \\dots, c_m\\}$. Заменим его на $m$ бинарных признаков $b_1(x), \\dots, b_m(x)$, каждый из которых является индикатором одного из возможных категориальных значений:\n",
    "$$\n",
    "b_i(x) = [f_j(x) = c_i]\n",
    "$$\n",
    "\n",
    "__Задание 1.__ Закодируйте все категориальные признаки с помощью one-hot-кодирования. Обучите логистическую регрессию и посмотрите, как изменилось качество модели (с тем, что было ранее). Измерьте время, потребовавшееся на обучение модели.\n",
    "\n",
    "__(3 балла)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "GC4tPzPbaRLW"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ps_ind_01</th>\n",
       "      <th>ps_ind_02_cat_-1</th>\n",
       "      <th>ps_ind_02_cat_1</th>\n",
       "      <th>ps_ind_02_cat_2</th>\n",
       "      <th>ps_ind_02_cat_3</th>\n",
       "      <th>ps_ind_02_cat_4</th>\n",
       "      <th>ps_ind_03</th>\n",
       "      <th>ps_ind_04_cat_-1</th>\n",
       "      <th>ps_ind_04_cat_0</th>\n",
       "      <th>ps_ind_04_cat_1</th>\n",
       "      <th>...</th>\n",
       "      <th>ps_calc_11</th>\n",
       "      <th>ps_calc_12</th>\n",
       "      <th>ps_calc_13</th>\n",
       "      <th>ps_calc_14</th>\n",
       "      <th>ps_calc_15_bin</th>\n",
       "      <th>ps_calc_16_bin</th>\n",
       "      <th>ps_calc_17_bin</th>\n",
       "      <th>ps_calc_18_bin</th>\n",
       "      <th>ps_calc_19_bin</th>\n",
       "      <th>ps_calc_20_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.00000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.998215</td>\n",
       "      <td>0.001070</td>\n",
       "      <td>0.718275</td>\n",
       "      <td>0.212700</td>\n",
       "      <td>0.047875</td>\n",
       "      <td>0.020080</td>\n",
       "      <td>4.483870</td>\n",
       "      <td>0.000835</td>\n",
       "      <td>0.56884</td>\n",
       "      <td>0.430325</td>\n",
       "      <td>...</td>\n",
       "      <td>5.443865</td>\n",
       "      <td>1.443745</td>\n",
       "      <td>2.873590</td>\n",
       "      <td>7.544455</td>\n",
       "      <td>0.123355</td>\n",
       "      <td>0.630875</td>\n",
       "      <td>0.553405</td>\n",
       "      <td>0.287530</td>\n",
       "      <td>0.345000</td>\n",
       "      <td>0.152800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.017199</td>\n",
       "      <td>0.032693</td>\n",
       "      <td>0.449841</td>\n",
       "      <td>0.409218</td>\n",
       "      <td>0.213502</td>\n",
       "      <td>0.140274</td>\n",
       "      <td>2.739255</td>\n",
       "      <td>0.028884</td>\n",
       "      <td>0.49524</td>\n",
       "      <td>0.495123</td>\n",
       "      <td>...</td>\n",
       "      <td>2.342462</td>\n",
       "      <td>1.201163</td>\n",
       "      <td>1.692875</td>\n",
       "      <td>2.745287</td>\n",
       "      <td>0.328845</td>\n",
       "      <td>0.482569</td>\n",
       "      <td>0.497141</td>\n",
       "      <td>0.452612</td>\n",
       "      <td>0.475369</td>\n",
       "      <td>0.359796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 226 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ps_ind_01  ps_ind_02_cat_-1  ps_ind_02_cat_1  ps_ind_02_cat_2  \\\n",
       "count  200000.000000     200000.000000    200000.000000    200000.000000   \n",
       "mean        1.998215          0.001070         0.718275         0.212700   \n",
       "std         2.017199          0.032693         0.449841         0.409218   \n",
       "min         0.000000          0.000000         0.000000         0.000000   \n",
       "25%         0.000000          0.000000         0.000000         0.000000   \n",
       "50%         1.000000          0.000000         1.000000         0.000000   \n",
       "75%         3.000000          0.000000         1.000000         0.000000   \n",
       "max         7.000000          1.000000         1.000000         1.000000   \n",
       "\n",
       "       ps_ind_02_cat_3  ps_ind_02_cat_4      ps_ind_03  ps_ind_04_cat_-1  \\\n",
       "count    200000.000000    200000.000000  200000.000000     200000.000000   \n",
       "mean          0.047875         0.020080       4.483870          0.000835   \n",
       "std           0.213502         0.140274       2.739255          0.028884   \n",
       "min           0.000000         0.000000       0.000000          0.000000   \n",
       "25%           0.000000         0.000000       2.000000          0.000000   \n",
       "50%           0.000000         0.000000       4.000000          0.000000   \n",
       "75%           0.000000         0.000000       7.000000          0.000000   \n",
       "max           1.000000         1.000000      11.000000          1.000000   \n",
       "\n",
       "       ps_ind_04_cat_0  ps_ind_04_cat_1  ...     ps_calc_11     ps_calc_12  \\\n",
       "count     200000.00000    200000.000000  ...  200000.000000  200000.000000   \n",
       "mean           0.56884         0.430325  ...       5.443865       1.443745   \n",
       "std            0.49524         0.495123  ...       2.342462       1.201163   \n",
       "min            0.00000         0.000000  ...       0.000000       0.000000   \n",
       "25%            0.00000         0.000000  ...       4.000000       1.000000   \n",
       "50%            1.00000         0.000000  ...       5.000000       1.000000   \n",
       "75%            1.00000         1.000000  ...       7.000000       2.000000   \n",
       "max            1.00000         1.000000  ...      18.000000       8.000000   \n",
       "\n",
       "          ps_calc_13     ps_calc_14  ps_calc_15_bin  ps_calc_16_bin  \\\n",
       "count  200000.000000  200000.000000   200000.000000   200000.000000   \n",
       "mean        2.873590       7.544455        0.123355        0.630875   \n",
       "std         1.692875       2.745287        0.328845        0.482569   \n",
       "min         0.000000       0.000000        0.000000        0.000000   \n",
       "25%         2.000000       6.000000        0.000000        0.000000   \n",
       "50%         3.000000       7.000000        0.000000        1.000000   \n",
       "75%         4.000000       9.000000        0.000000        1.000000   \n",
       "max        13.000000      22.000000        1.000000        1.000000   \n",
       "\n",
       "       ps_calc_17_bin  ps_calc_18_bin  ps_calc_19_bin  ps_calc_20_bin  \n",
       "count   200000.000000   200000.000000   200000.000000   200000.000000  \n",
       "mean         0.553405        0.287530        0.345000        0.152800  \n",
       "std          0.497141        0.452612        0.475369        0.359796  \n",
       "min          0.000000        0.000000        0.000000        0.000000  \n",
       "25%          0.000000        0.000000        0.000000        0.000000  \n",
       "50%          1.000000        0.000000        0.000000        0.000000  \n",
       "75%          1.000000        1.000000        1.000000        0.000000  \n",
       "max          1.000000        1.000000        1.000000        1.000000  \n",
       "\n",
       "[8 rows x 226 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one-hot encoding\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "data = start_data.copy()\n",
    "\n",
    "def transform_data(data):\n",
    "    new_data = pd.DataFrame()\n",
    "\n",
    "    for v in data:\n",
    "        if v[-3:] == \"cat\": # переменная категориальная\n",
    "            categories = dict()\n",
    "            uniq = set()\n",
    "            ptr = 0\n",
    "\n",
    "            for x in data[v]:\n",
    "                uniq.add(x)\n",
    "\n",
    "            uniq = sorted(uniq)\n",
    "\n",
    "            for value in uniq:\n",
    "                categories[value] = ptr\n",
    "                ptr += 1\n",
    "\n",
    "            ptr = 0\n",
    "\n",
    "            for value in uniq:\n",
    "                arr = []\n",
    "                for x in data[v]:\n",
    "                    if x == value:\n",
    "                        arr.append(1)\n",
    "                    else:\n",
    "                        arr.append(0)\n",
    "                \n",
    "                ptr += 1\n",
    "\n",
    "                new_data[v + \"_\" + str(value)] = arr\n",
    "        else:\n",
    "            new_data[v] = data[v]\n",
    "    \n",
    "    return new_data\n",
    "\n",
    "OneHotTransformed_data = transform_data(data)\n",
    "\n",
    "OneHotTransformed_data.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "OneHotTransformed_data = normalize_data(OneHotTransformed_data)\n",
    "\n",
    "# Итак, у нас есть new_data, обучим logistic regression\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(OneHotTransformed_data, target, test_size=0.5, random_state=46)\n",
    "\n",
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2.3 s\n",
      "Wall time: 1.99 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy =  0.59666 precision =  0.6051999313540415 recall =  0.5626420964460931 F_score =  0.5831455796936688\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(X_test)\n",
    "\n",
    "accuracy, precision, recall, F_score = get_info_prediction(prediction, y_test)\n",
    "\n",
    "print(\"accuracy = \", accuracy, \"precision = \", precision, \"recall = \", recall, \"F_score = \", F_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Короче я не знал, что есть OneHotEncoder, сейчас попробуем с ним"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "OneHotEncoder_data = start_data.copy()\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False)  # You can customize options as needed\n",
    "\n",
    "categories = []\n",
    "\n",
    "for v in data:\n",
    "    if v[-3:] == \"cat\":\n",
    "        categories.append(v)\n",
    "\n",
    "data_categorial = data[categories].copy()\n",
    "\n",
    "encoded_data = encoder.fit_transform(data_categorial)\n",
    "\n",
    "encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(categories))\n",
    "\n",
    "OneHotEncoder_data.drop(columns=categories, inplace=True)  # Remove original categorical columns\n",
    "\n",
    "OneHotEncoder_data = OneHotEncoder_data.reset_index(drop=True)  # Reset index and drop the old index\n",
    "encoded_df = encoded_df.reset_index(drop=True)\n",
    "\n",
    "OneHotEncoder_data = pd.concat([OneHotEncoder_data, encoded_df], axis=1) \n",
    "\n",
    "OneHotEncoder_data = normalize_data(OneHotEncoder_data)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(OneHotEncoder_data, target, test_size=0.5, random_state=46)\n",
    "\n",
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2 s\n",
      "Wall time: 1.99 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy =  0.59666 precision =  0.6051999313540415 recall =  0.5626420964460931 F_score =  0.5831455796936688\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(X_test)\n",
    "\n",
    "accuracy, precision, recall, F_score = get_info_prediction(prediction, y_test)\n",
    "\n",
    "print(\"accuracy = \", accuracy, \"precision = \", precision, \"recall = \", recall, \"F_score = \", F_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мини - вывод: OneHotEncoder работает также самописная версия :), но в целом OneHotEncoding улучшило результаты (accuracy улушилась на 0.5%, recall на ~1.2% и F_score на ~0.9%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "izITXcOWaRLW"
   },
   "source": [
    "Как можно было заменить, one-hot-кодирование может сильно увеличивать количество признаков в датасете, что сказывается на памяти, особенно, если некоторый признак имеет большое количество значений. Эту проблему решает другой способ кодирование категориальных признаков — счётчики. Основная идея в том, что нам важны не сами категории, а значения целевой переменной, которые имеют объекты этой категории. Каждый категориальный признак мы заменим средним значением целевой переменной по всем объектам этой же категории:\n",
    "$$\n",
    "g_j(x, X) = \\frac{\\sum_{i=1}^{l} [f_j(x) = f_j(x_i)][y_i = +1]}{\\sum_{i=1}^{l} [f_j(x) = f_j(x_i)]}\n",
    "$$\n",
    "\n",
    "__Задание 2.__ Закодируйте категориальные переменные с помощью счётчиков (ровно так, как описано выше без каких-либо хитростей). Обучите логистическую регрессию и посмотрите на качество модели на тестовом множестве. Сравните время обучения с предыдущим экспериментов. Заметили ли вы что-то интересное?\n",
    "\n",
    "__(2 балла)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "vJmhJjcyaRLW"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ps_ind_01</th>\n",
       "      <th>ps_ind_02_cat</th>\n",
       "      <th>ps_ind_03</th>\n",
       "      <th>ps_ind_04_cat</th>\n",
       "      <th>ps_ind_05_cat</th>\n",
       "      <th>ps_ind_06_bin</th>\n",
       "      <th>ps_ind_07_bin</th>\n",
       "      <th>ps_ind_08_bin</th>\n",
       "      <th>ps_ind_09_bin</th>\n",
       "      <th>ps_ind_10_bin</th>\n",
       "      <th>...</th>\n",
       "      <th>ps_calc_11</th>\n",
       "      <th>ps_calc_12</th>\n",
       "      <th>ps_calc_13</th>\n",
       "      <th>ps_calc_14</th>\n",
       "      <th>ps_calc_15_bin</th>\n",
       "      <th>ps_calc_16_bin</th>\n",
       "      <th>ps_calc_17_bin</th>\n",
       "      <th>ps_calc_18_bin</th>\n",
       "      <th>ps_calc_19_bin</th>\n",
       "      <th>ps_calc_20_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.998215</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>4.483870</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.351100</td>\n",
       "      <td>0.295375</td>\n",
       "      <td>0.175920</td>\n",
       "      <td>0.177605</td>\n",
       "      <td>0.000470</td>\n",
       "      <td>...</td>\n",
       "      <td>5.443865</td>\n",
       "      <td>1.443745</td>\n",
       "      <td>2.873590</td>\n",
       "      <td>7.544455</td>\n",
       "      <td>0.123355</td>\n",
       "      <td>0.630875</td>\n",
       "      <td>0.553405</td>\n",
       "      <td>0.287530</td>\n",
       "      <td>0.345000</td>\n",
       "      <td>0.152800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.017199</td>\n",
       "      <td>0.015959</td>\n",
       "      <td>2.739255</td>\n",
       "      <td>0.020419</td>\n",
       "      <td>0.050252</td>\n",
       "      <td>0.477315</td>\n",
       "      <td>0.456212</td>\n",
       "      <td>0.380753</td>\n",
       "      <td>0.382181</td>\n",
       "      <td>0.021674</td>\n",
       "      <td>...</td>\n",
       "      <td>2.342462</td>\n",
       "      <td>1.201163</td>\n",
       "      <td>1.692875</td>\n",
       "      <td>2.745287</td>\n",
       "      <td>0.328845</td>\n",
       "      <td>0.482569</td>\n",
       "      <td>0.497141</td>\n",
       "      <td>0.452612</td>\n",
       "      <td>0.475369</td>\n",
       "      <td>0.359796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.494915</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.486218</td>\n",
       "      <td>0.480547</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.494915</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.486218</td>\n",
       "      <td>0.480547</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.494915</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.486218</td>\n",
       "      <td>0.480547</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.500783</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.517318</td>\n",
       "      <td>0.480547</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.911215</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.964072</td>\n",
       "      <td>0.697423</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ps_ind_01  ps_ind_02_cat      ps_ind_03  ps_ind_04_cat  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean        1.998215       0.500000       4.483870       0.500000   \n",
       "std         2.017199       0.015959       2.739255       0.020419   \n",
       "min         0.000000       0.494915       0.000000       0.486218   \n",
       "25%         0.000000       0.494915       2.000000       0.486218   \n",
       "50%         1.000000       0.494915       4.000000       0.486218   \n",
       "75%         3.000000       0.500783       7.000000       0.517318   \n",
       "max         7.000000       0.911215      11.000000       0.964072   \n",
       "\n",
       "       ps_ind_05_cat  ps_ind_06_bin  ps_ind_07_bin  ps_ind_08_bin  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean        0.500000       0.351100       0.295375       0.175920   \n",
       "std         0.050252       0.477315       0.456212       0.380753   \n",
       "min         0.480547       0.000000       0.000000       0.000000   \n",
       "25%         0.480547       0.000000       0.000000       0.000000   \n",
       "50%         0.480547       0.000000       0.000000       0.000000   \n",
       "75%         0.480547       1.000000       1.000000       0.000000   \n",
       "max         0.697423       1.000000       1.000000       1.000000   \n",
       "\n",
       "       ps_ind_09_bin  ps_ind_10_bin  ...     ps_calc_11     ps_calc_12  \\\n",
       "count  200000.000000  200000.000000  ...  200000.000000  200000.000000   \n",
       "mean        0.177605       0.000470  ...       5.443865       1.443745   \n",
       "std         0.382181       0.021674  ...       2.342462       1.201163   \n",
       "min         0.000000       0.000000  ...       0.000000       0.000000   \n",
       "25%         0.000000       0.000000  ...       4.000000       1.000000   \n",
       "50%         0.000000       0.000000  ...       5.000000       1.000000   \n",
       "75%         0.000000       0.000000  ...       7.000000       2.000000   \n",
       "max         1.000000       1.000000  ...      18.000000       8.000000   \n",
       "\n",
       "          ps_calc_13     ps_calc_14  ps_calc_15_bin  ps_calc_16_bin  \\\n",
       "count  200000.000000  200000.000000   200000.000000   200000.000000   \n",
       "mean        2.873590       7.544455        0.123355        0.630875   \n",
       "std         1.692875       2.745287        0.328845        0.482569   \n",
       "min         0.000000       0.000000        0.000000        0.000000   \n",
       "25%         2.000000       6.000000        0.000000        0.000000   \n",
       "50%         3.000000       7.000000        0.000000        1.000000   \n",
       "75%         4.000000       9.000000        0.000000        1.000000   \n",
       "max        13.000000      22.000000        1.000000        1.000000   \n",
       "\n",
       "       ps_calc_17_bin  ps_calc_18_bin  ps_calc_19_bin  ps_calc_20_bin  \n",
       "count   200000.000000   200000.000000   200000.000000   200000.000000  \n",
       "mean         0.553405        0.287530        0.345000        0.152800  \n",
       "std          0.497141        0.452612        0.475369        0.359796  \n",
       "min          0.000000        0.000000        0.000000        0.000000  \n",
       "25%          0.000000        0.000000        0.000000        0.000000  \n",
       "50%          1.000000        0.000000        0.000000        0.000000  \n",
       "75%          1.000000        1.000000        1.000000        0.000000  \n",
       "max          1.000000        1.000000        1.000000        1.000000  \n",
       "\n",
       "[8 rows x 57 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CTR_data = start_data.copy()\n",
    "\n",
    "for v in start_data:\n",
    "    if v in categories:\n",
    "        uniq = np.unique(data[v])\n",
    "        for value in uniq:\n",
    "            CTR_data[v][start_data[v] == value] = np.mean(target[start_data[v] == value])\n",
    "\n",
    "CTR_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "CTR_data = normalize_data(CTR_data)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(CTR_data, target, test_size=0.5, random_state=46)\n",
    "\n",
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 156 ms\n",
      "Wall time: 278 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-8 {color: black;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" checked><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy =  0.59449 precision =  0.602155805977462 recall =  0.5637389812931275 F_score =  0.5823144667044342\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(X_test)\n",
    "\n",
    "accuracy, precision, recall, F_score = get_info_prediction(prediction, y_test)\n",
    "\n",
    "print(\"accuracy = \", accuracy, \"precision = \", precision, \"recall = \", recall, \"F_score = \", F_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CZ6BybtVaRLW"
   },
   "source": [
    "__Вывод:__ При использовании счетчиков(CTR) и OneHotEncoding мы получаем похожие метрики, отличааются на ~0.1-0.3% (precision лучше у OneHotEncoding, а recall у CTR), но при этом обучение при использовании счетчиков тратит гораздо меньше времени (0.4ms VS 5s), чем OneHotEncoding. Очевидно, что это связано с тем, что при использовании OneHotEncoding количество данных значительно больше, чем при использовании CTR, из-за этого мы и видим разницу в скорости обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IUSCGlbhaRLW"
   },
   "source": [
    "Отметим, что такие признаки сами по себе являются классификаторами и, обучаясь на них, мы допускаем \"утечку\" целевой переменной в признаки. Это ведёт к переобучению, поэтому считать такие признаки необходимо таким образом, чтобы при вычислении для конкретного объекта его целевая метка не использовалась. Это можно делать следующими способами:\n",
    "- вычислять значение счётчика по всем объектам расположенным выше в датасете (например, если у нас выборка отсортирована по времени)\n",
    "- вычислять по фолдам, то есть делить выборку на некоторое количество частей и подсчитывать значение признаков по всем фолдам кроме текущего (как делается в кросс-валидации)\n",
    "- внесение некоторого шума в посчитанные признаки (необходимо соблюсти баланс между избавление от переобучения и полезностью признаков).\n",
    "\n",
    "__Задание 3.__ Реализуйте корректное вычисление счётчиков двумя из трех вышеперчисленных способов, сравните. Снова обучите логистическую регрессию, оцените качество. Сделайте выводы.\n",
    "\n",
    "__(3 балла)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем счетчики вычислять по фолдам (разбиваем на cnt_blocks блоков)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "YX9gBIEJaRLW"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ps_ind_01</th>\n",
       "      <th>ps_ind_02_cat</th>\n",
       "      <th>ps_ind_03</th>\n",
       "      <th>ps_ind_04_cat</th>\n",
       "      <th>ps_ind_05_cat</th>\n",
       "      <th>ps_ind_06_bin</th>\n",
       "      <th>ps_ind_07_bin</th>\n",
       "      <th>ps_ind_08_bin</th>\n",
       "      <th>ps_ind_09_bin</th>\n",
       "      <th>ps_ind_10_bin</th>\n",
       "      <th>...</th>\n",
       "      <th>ps_calc_11</th>\n",
       "      <th>ps_calc_12</th>\n",
       "      <th>ps_calc_13</th>\n",
       "      <th>ps_calc_14</th>\n",
       "      <th>ps_calc_15_bin</th>\n",
       "      <th>ps_calc_16_bin</th>\n",
       "      <th>ps_calc_17_bin</th>\n",
       "      <th>ps_calc_18_bin</th>\n",
       "      <th>ps_calc_19_bin</th>\n",
       "      <th>ps_calc_20_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.998215</td>\n",
       "      <td>0.500325</td>\n",
       "      <td>4.483870</td>\n",
       "      <td>0.500340</td>\n",
       "      <td>0.500336</td>\n",
       "      <td>0.351100</td>\n",
       "      <td>0.295375</td>\n",
       "      <td>0.175920</td>\n",
       "      <td>0.177605</td>\n",
       "      <td>0.000470</td>\n",
       "      <td>...</td>\n",
       "      <td>5.443865</td>\n",
       "      <td>1.443745</td>\n",
       "      <td>2.873590</td>\n",
       "      <td>7.544455</td>\n",
       "      <td>0.123355</td>\n",
       "      <td>0.630875</td>\n",
       "      <td>0.553405</td>\n",
       "      <td>0.287530</td>\n",
       "      <td>0.345000</td>\n",
       "      <td>0.152800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.017199</td>\n",
       "      <td>0.015949</td>\n",
       "      <td>2.739255</td>\n",
       "      <td>0.020522</td>\n",
       "      <td>0.050333</td>\n",
       "      <td>0.477315</td>\n",
       "      <td>0.456212</td>\n",
       "      <td>0.380753</td>\n",
       "      <td>0.382181</td>\n",
       "      <td>0.021674</td>\n",
       "      <td>...</td>\n",
       "      <td>2.342462</td>\n",
       "      <td>1.201163</td>\n",
       "      <td>1.692875</td>\n",
       "      <td>2.745287</td>\n",
       "      <td>0.328845</td>\n",
       "      <td>0.482569</td>\n",
       "      <td>0.497141</td>\n",
       "      <td>0.452612</td>\n",
       "      <td>0.475369</td>\n",
       "      <td>0.359796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.495397</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.486583</td>\n",
       "      <td>0.480839</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.495397</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.486583</td>\n",
       "      <td>0.480839</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.495397</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.486583</td>\n",
       "      <td>0.480839</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.502319</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.517610</td>\n",
       "      <td>0.480839</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.915730</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.971223</td>\n",
       "      <td>0.698358</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ps_ind_01  ps_ind_02_cat      ps_ind_03  ps_ind_04_cat  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean        1.998215       0.500325       4.483870       0.500340   \n",
       "std         2.017199       0.015949       2.739255       0.020522   \n",
       "min         0.000000       0.495397       0.000000       0.486583   \n",
       "25%         0.000000       0.495397       2.000000       0.486583   \n",
       "50%         1.000000       0.495397       4.000000       0.486583   \n",
       "75%         3.000000       0.502319       7.000000       0.517610   \n",
       "max         7.000000       0.915730      11.000000       0.971223   \n",
       "\n",
       "       ps_ind_05_cat  ps_ind_06_bin  ps_ind_07_bin  ps_ind_08_bin  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean        0.500336       0.351100       0.295375       0.175920   \n",
       "std         0.050333       0.477315       0.456212       0.380753   \n",
       "min         0.480839       0.000000       0.000000       0.000000   \n",
       "25%         0.480839       0.000000       0.000000       0.000000   \n",
       "50%         0.480839       0.000000       0.000000       0.000000   \n",
       "75%         0.480839       1.000000       1.000000       0.000000   \n",
       "max         0.698358       1.000000       1.000000       1.000000   \n",
       "\n",
       "       ps_ind_09_bin  ps_ind_10_bin  ...     ps_calc_11     ps_calc_12  \\\n",
       "count  200000.000000  200000.000000  ...  200000.000000  200000.000000   \n",
       "mean        0.177605       0.000470  ...       5.443865       1.443745   \n",
       "std         0.382181       0.021674  ...       2.342462       1.201163   \n",
       "min         0.000000       0.000000  ...       0.000000       0.000000   \n",
       "25%         0.000000       0.000000  ...       4.000000       1.000000   \n",
       "50%         0.000000       0.000000  ...       5.000000       1.000000   \n",
       "75%         0.000000       0.000000  ...       7.000000       2.000000   \n",
       "max         1.000000       1.000000  ...      18.000000       8.000000   \n",
       "\n",
       "          ps_calc_13     ps_calc_14  ps_calc_15_bin  ps_calc_16_bin  \\\n",
       "count  200000.000000  200000.000000   200000.000000   200000.000000   \n",
       "mean        2.873590       7.544455        0.123355        0.630875   \n",
       "std         1.692875       2.745287        0.328845        0.482569   \n",
       "min         0.000000       0.000000        0.000000        0.000000   \n",
       "25%         2.000000       6.000000        0.000000        0.000000   \n",
       "50%         3.000000       7.000000        0.000000        1.000000   \n",
       "75%         4.000000       9.000000        0.000000        1.000000   \n",
       "max        13.000000      22.000000        1.000000        1.000000   \n",
       "\n",
       "       ps_calc_17_bin  ps_calc_18_bin  ps_calc_19_bin  ps_calc_20_bin  \n",
       "count   200000.000000   200000.000000   200000.000000   200000.000000  \n",
       "mean         0.553405        0.287530        0.345000        0.152800  \n",
       "std          0.497141        0.452612        0.475369        0.359796  \n",
       "min          0.000000        0.000000        0.000000        0.000000  \n",
       "25%          0.000000        0.000000        0.000000        0.000000  \n",
       "50%          1.000000        0.000000        0.000000        0.000000  \n",
       "75%          1.000000        1.000000        1.000000        0.000000  \n",
       "max          1.000000        1.000000        1.000000        1.000000  \n",
       "\n",
       "[8 rows x 57 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold_data = start_data.copy()\n",
    "\n",
    "def get_sum (v, index, cumsum, cnt_blocks, n):\n",
    "    part = n // cnt_blocks + 1\n",
    "    l = (index // part) * part\n",
    "    r = min(n, (index // part + 1) * (part))\n",
    "    length = l + (n - r)\n",
    "    sum = cumsum[l] + cumsum[n] - cumsum[r - 1]\n",
    "    return sum / length\n",
    "\n",
    "for v in categories:\n",
    "    cnt_blocks = 6\n",
    "    ans = []\n",
    "\n",
    "    uniq = np.unique(data[v])\n",
    "    cumsum = dict()\n",
    "    cnt = dict()\n",
    "\n",
    "    for u in uniq:\n",
    "        np_data = np.array(target[data[v] == u])\n",
    "        cumsum[u] = np.hstack((np.array([0]), np.cumsum(np_data)))\n",
    "    \n",
    "    for value in data[v]:\n",
    "        if not value in cnt.items():\n",
    "            cnt[value] = 0\n",
    "        ans.append(get_sum(value, cnt[value], cumsum[value], cnt_blocks, len(cumsum[value]) - 1))\n",
    "        cnt[value] += 1\n",
    "\n",
    "    fold_data[v] = pd.Series(ans, index=data[v].index)\n",
    "\n",
    "fold_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_data = normalize_data(fold_data)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(fold_data, target, test_size=0.5, random_state=46)\n",
    "\n",
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 234 ms\n",
      "Wall time: 288 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-9 {color: black;}#sk-container-id-9 pre{padding: 0;}#sk-container-id-9 div.sk-toggleable {background-color: white;}#sk-container-id-9 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-9 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-9 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-9 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-9 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-9 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-9 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-9 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-9 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-9 div.sk-item {position: relative;z-index: 1;}#sk-container-id-9 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-9 div.sk-item::before, #sk-container-id-9 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-9 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-9 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-9 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-9 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-9 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-9 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-9 div.sk-label-container {text-align: center;}#sk-container-id-9 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-9 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-9\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" checked><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy =  0.59412 precision =  0.6017985764821208 recall =  0.563200510550038 F_score =  0.581860139283801\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(X_test)\n",
    "\n",
    "accuracy, precision, recall, F_score = get_info_prediction(prediction, y_test)\n",
    "\n",
    "print(\"accuracy = \", accuracy, \"precision = \", precision, \"recall = \", recall, \"F_score = \", F_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем добавить немного шума счетчикам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ps_ind_01</th>\n",
       "      <th>ps_ind_02_cat</th>\n",
       "      <th>ps_ind_03</th>\n",
       "      <th>ps_ind_04_cat</th>\n",
       "      <th>ps_ind_05_cat</th>\n",
       "      <th>ps_ind_06_bin</th>\n",
       "      <th>ps_ind_07_bin</th>\n",
       "      <th>ps_ind_08_bin</th>\n",
       "      <th>ps_ind_09_bin</th>\n",
       "      <th>ps_ind_10_bin</th>\n",
       "      <th>...</th>\n",
       "      <th>ps_calc_11</th>\n",
       "      <th>ps_calc_12</th>\n",
       "      <th>ps_calc_13</th>\n",
       "      <th>ps_calc_14</th>\n",
       "      <th>ps_calc_15_bin</th>\n",
       "      <th>ps_calc_16_bin</th>\n",
       "      <th>ps_calc_17_bin</th>\n",
       "      <th>ps_calc_18_bin</th>\n",
       "      <th>ps_calc_19_bin</th>\n",
       "      <th>ps_calc_20_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.998215</td>\n",
       "      <td>0.505002</td>\n",
       "      <td>4.483870</td>\n",
       "      <td>0.504997</td>\n",
       "      <td>0.505009</td>\n",
       "      <td>0.351100</td>\n",
       "      <td>0.295375</td>\n",
       "      <td>0.175920</td>\n",
       "      <td>0.177605</td>\n",
       "      <td>0.000470</td>\n",
       "      <td>...</td>\n",
       "      <td>5.443865</td>\n",
       "      <td>1.443745</td>\n",
       "      <td>2.873590</td>\n",
       "      <td>7.544455</td>\n",
       "      <td>0.123355</td>\n",
       "      <td>0.630875</td>\n",
       "      <td>0.553405</td>\n",
       "      <td>0.287530</td>\n",
       "      <td>0.345000</td>\n",
       "      <td>0.152800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.017199</td>\n",
       "      <td>0.016215</td>\n",
       "      <td>2.739255</td>\n",
       "      <td>0.020622</td>\n",
       "      <td>0.050332</td>\n",
       "      <td>0.477315</td>\n",
       "      <td>0.456212</td>\n",
       "      <td>0.380753</td>\n",
       "      <td>0.382181</td>\n",
       "      <td>0.021674</td>\n",
       "      <td>...</td>\n",
       "      <td>2.342462</td>\n",
       "      <td>1.201163</td>\n",
       "      <td>1.692875</td>\n",
       "      <td>2.745287</td>\n",
       "      <td>0.328845</td>\n",
       "      <td>0.482569</td>\n",
       "      <td>0.497141</td>\n",
       "      <td>0.452612</td>\n",
       "      <td>0.475369</td>\n",
       "      <td>0.359796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.494915</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.486218</td>\n",
       "      <td>0.480547</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.498400</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.490607</td>\n",
       "      <td>0.483476</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.501814</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.495000</td>\n",
       "      <td>0.486395</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.507395</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.521512</td>\n",
       "      <td>0.489310</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.921214</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.974049</td>\n",
       "      <td>0.707421</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ps_ind_01  ps_ind_02_cat      ps_ind_03  ps_ind_04_cat  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean        1.998215       0.505002       4.483870       0.504997   \n",
       "std         2.017199       0.016215       2.739255       0.020622   \n",
       "min         0.000000       0.494915       0.000000       0.486218   \n",
       "25%         0.000000       0.498400       2.000000       0.490607   \n",
       "50%         1.000000       0.501814       4.000000       0.495000   \n",
       "75%         3.000000       0.507395       7.000000       0.521512   \n",
       "max         7.000000       0.921214      11.000000       0.974049   \n",
       "\n",
       "       ps_ind_05_cat  ps_ind_06_bin  ps_ind_07_bin  ps_ind_08_bin  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean        0.505009       0.351100       0.295375       0.175920   \n",
       "std         0.050332       0.477315       0.456212       0.380753   \n",
       "min         0.480547       0.000000       0.000000       0.000000   \n",
       "25%         0.483476       0.000000       0.000000       0.000000   \n",
       "50%         0.486395       0.000000       0.000000       0.000000   \n",
       "75%         0.489310       1.000000       1.000000       0.000000   \n",
       "max         0.707421       1.000000       1.000000       1.000000   \n",
       "\n",
       "       ps_ind_09_bin  ps_ind_10_bin  ...     ps_calc_11     ps_calc_12  \\\n",
       "count  200000.000000  200000.000000  ...  200000.000000  200000.000000   \n",
       "mean        0.177605       0.000470  ...       5.443865       1.443745   \n",
       "std         0.382181       0.021674  ...       2.342462       1.201163   \n",
       "min         0.000000       0.000000  ...       0.000000       0.000000   \n",
       "25%         0.000000       0.000000  ...       4.000000       1.000000   \n",
       "50%         0.000000       0.000000  ...       5.000000       1.000000   \n",
       "75%         0.000000       0.000000  ...       7.000000       2.000000   \n",
       "max         1.000000       1.000000  ...      18.000000       8.000000   \n",
       "\n",
       "          ps_calc_13     ps_calc_14  ps_calc_15_bin  ps_calc_16_bin  \\\n",
       "count  200000.000000  200000.000000   200000.000000   200000.000000   \n",
       "mean        2.873590       7.544455        0.123355        0.630875   \n",
       "std         1.692875       2.745287        0.328845        0.482569   \n",
       "min         0.000000       0.000000        0.000000        0.000000   \n",
       "25%         2.000000       6.000000        0.000000        0.000000   \n",
       "50%         3.000000       7.000000        0.000000        1.000000   \n",
       "75%         4.000000       9.000000        0.000000        1.000000   \n",
       "max        13.000000      22.000000        1.000000        1.000000   \n",
       "\n",
       "       ps_calc_17_bin  ps_calc_18_bin  ps_calc_19_bin  ps_calc_20_bin  \n",
       "count   200000.000000   200000.000000   200000.000000   200000.000000  \n",
       "mean         0.553405        0.287530        0.345000        0.152800  \n",
       "std          0.497141        0.452612        0.475369        0.359796  \n",
       "min          0.000000        0.000000        0.000000        0.000000  \n",
       "25%          0.000000        0.000000        0.000000        0.000000  \n",
       "50%          1.000000        0.000000        0.000000        0.000000  \n",
       "75%          1.000000        1.000000        1.000000        0.000000  \n",
       "max          1.000000        1.000000        1.000000        1.000000  \n",
       "\n",
       "[8 rows x 57 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CTR_noise_data = start_data.copy()\n",
    "\n",
    "for v in start_data:\n",
    "    if v in categories:\n",
    "        uniq = np.unique(data[v])\n",
    "        for value in uniq:\n",
    "            current = np.array(target[start_data[v] == value])\n",
    "            CTR_noise_data[v][start_data[v] == value] = np.mean(current) + np.random.random(current.shape) * 0.01\n",
    "\n",
    "CTR_noise_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "CTR_noise_data = normalize_data(CTR_noise_data)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(CTR_noise_data, target, test_size=0.5, random_state=46)\n",
    "\n",
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 203 ms\n",
      "Wall time: 310 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-10 {color: black;}#sk-container-id-10 pre{padding: 0;}#sk-container-id-10 div.sk-toggleable {background-color: white;}#sk-container-id-10 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-10 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-10 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-10 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-10 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-10 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-10 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-10 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-10 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-10 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-10 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-10 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-10 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-10 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-10 div.sk-item {position: relative;z-index: 1;}#sk-container-id-10 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-10 div.sk-item::before, #sk-container-id-10 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-10 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-10 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-10 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-10 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-10 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-10 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-10 div.sk-label-container {text-align: center;}#sk-container-id-10 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-10 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-10\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" checked><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy =  0.59427 precision =  0.6018607225735028 recall =  0.563798811375693 F_score =  0.5822083552150588\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(X_test)\n",
    "\n",
    "accuracy, precision, recall, F_score = get_info_prediction(prediction, y_test)\n",
    "\n",
    "print(\"accuracy = \", accuracy, \"precision = \", precision, \"recall = \", recall, \"F_score = \", F_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CIXbvzlWaRLX"
   },
   "source": [
    "__Вывод:__ Разбиение по фолдам и добавление шума вроде как уменьшает все метрики на ~0-0.3%, но с другой стороны эти приемы дают модели не переобучаться и возможно на реальных данных такая модель будет иметь лучший результат."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cbde96fnaRLV"
   },
   "source": [
    "## Часть 2. Метод опорных векторов и калибровка вероятностней"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mvqNBbT4aRLV"
   },
   "source": [
    "__Задание 1.__ Обучение и применение метода опорных векторов.\n",
    "\n",
    "__(1 балл)__\n",
    "\n",
    "Обучите метод опорных векторов (воспользуйтесь готовой реализацией LinearSVC из sklearn). Используйте уже загруженные и обработанные в предыдущей части данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "G4eGtEwzaRLV"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svc = LinearSVC(random_state=123)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(CTR_noise_data, target, test_size=0.5, random_state=46)\n",
    "\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "prediction = svc.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "reSZTM1daRLV"
   },
   "source": [
    "На той же тестовой части посчитайте все те же метрики. Что вы можете сказать о полученных результатах?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "0jW0b67TaRLV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy =  0.59397 precision =  0.602316850799099 recall =  0.559949742730645 F_score =  0.5803611108240231\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, recall, F_score = get_info_prediction(prediction, y_test)\n",
    "\n",
    "print(\"accuracy = \", accuracy, \"precision = \", precision, \"recall = \", recall, \"F_score = \", F_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ECd18zToaRLW"
   },
   "source": [
    "__Вывод:__ Метод опорных векторов показывает примерно такие же метрики, как и логистическая регрессия на предыдущих данных, но при этом обучение модели занимает примерно 50 секунд, в отличие от метода счетчиков(0.4 секунды в среднем)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Report__: Самой лучшей моделью по метрикам оказались просто модель с использованием счетчиков (без ничего). Но остальные модели, кроме обычной логистической регрессии (без изменения данных), имели очень похожие метрики (разница ~0.1-0.3%). "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
